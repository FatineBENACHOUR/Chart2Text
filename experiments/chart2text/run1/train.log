INFO - 04/25/22 12:36:49 - 0:00:00 - ============ Initialized logger ============
INFO - 04/25/22 12:36:49 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: True
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments\chart2text\run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: True
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 04/25/22 12:36:49 - 0:00:00 - The experiment will be stored in experiments\chart2text\run1
                                     
INFO - 04/25/22 12:36:49 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings True --encoder_positional_emb True --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 04/25/22 12:36:49 - 0:00:00 - ============ Data summary ============
INFO - 04/25/22 12:36:49 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 12:36:49 - 0:00:00 - Removed 0 empty sentences.

INFO - 04/25/22 12:36:49 - 0:00:00 - Content-Selection Data -       5703
INFO - 04/25/22 12:36:49 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 12:36:49 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 04/25/22 12:36:49 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 12:36:49 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 12:36:49 - 0:00:00 - Para Data          -       5703
INFO - 04/25/22 12:36:49 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 04/25/22 12:36:49 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 04/25/22 12:36:49 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 12:36:49 - 0:00:00 - Removed 0 empty sentences.

INFO - 04/25/22 13:00:50 - 0:00:00 - ============ Initialized logger ============
INFO - 04/25/22 13:00:50 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: False
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments\chart2text\run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: False
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 04/25/22 13:00:50 - 0:00:00 - The experiment will be stored in experiments\chart2text\run1
                                     
INFO - 04/25/22 13:00:50 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 04/25/22 13:00:50 - 0:00:00 - ============ Data summary ============
INFO - 04/25/22 13:00:50 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:00:50 - 0:00:00 - Removed 0 empty sentences.

INFO - 04/25/22 13:00:50 - 0:00:00 - Content-Selection Data -       5703
INFO - 04/25/22 13:00:50 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:00:50 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 04/25/22 13:00:50 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:00:50 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:00:50 - 0:00:00 - Para Data          -       5703
INFO - 04/25/22 13:00:50 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 04/25/22 13:00:50 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 04/25/22 13:00:50 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:00:50 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 04/25/22 13:00:51 - 0:00:00 - Encoder: TransformerEncoder(
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 04/25/22 13:00:51 - 0:00:00 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 04/25/22 13:00:51 - 0:00:00 - Number of parameters (encoder): 10563201
INFO - 04/25/22 13:00:51 - 0:00:00 - Number of parameters (decoder): 31278014
INFO - 04/25/22 13:09:51 - 0:00:00 - ============ Initialized logger ============
INFO - 04/25/22 13:09:51 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: False
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments\chart2text\run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: False
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 04/25/22 13:09:51 - 0:00:00 - The experiment will be stored in experiments\chart2text\run1
                                     
INFO - 04/25/22 13:09:51 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 04/25/22 13:09:51 - 0:00:00 - ============ Data summary ============
INFO - 04/25/22 13:09:51 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:09:51 - 0:00:00 - Removed 0 empty sentences.

INFO - 04/25/22 13:09:51 - 0:00:00 - Content-Selection Data -       5703
INFO - 04/25/22 13:09:51 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:09:51 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 04/25/22 13:09:51 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:09:51 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:09:51 - 0:00:00 - Para Data          -       5703
INFO - 04/25/22 13:09:51 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 04/25/22 13:09:51 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 04/25/22 13:09:51 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:09:51 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 04/25/22 13:09:52 - 0:00:00 - Encoder: TransformerEncoder(
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 04/25/22 13:09:52 - 0:00:00 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 04/25/22 13:09:52 - 0:00:00 - Number of parameters (encoder): 10563201
INFO - 04/25/22 13:09:52 - 0:00:00 - Number of parameters (decoder): 31278014
INFO - 04/25/22 13:12:39 - 0:00:00 - ============ Initialized logger ============
INFO - 04/25/22 13:12:39 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: False
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments\chart2text\run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: False
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 04/25/22 13:12:39 - 0:00:00 - The experiment will be stored in experiments\chart2text\run1
                                     
INFO - 04/25/22 13:12:39 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 04/25/22 13:12:39 - 0:00:00 - ============ Data summary ============
INFO - 04/25/22 13:12:39 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:12:39 - 0:00:00 - Removed 0 empty sentences.

INFO - 04/25/22 13:12:39 - 0:00:00 - Content-Selection Data -       5703
INFO - 04/25/22 13:12:39 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:12:39 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 04/25/22 13:12:39 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:12:39 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:12:39 - 0:00:00 - Para Data          -       5703
INFO - 04/25/22 13:12:39 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 04/25/22 13:12:39 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 04/25/22 13:12:39 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:12:39 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 04/25/22 13:12:40 - 0:00:00 - Encoder: TransformerEncoder(
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 04/25/22 13:12:40 - 0:00:00 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 04/25/22 13:12:40 - 0:00:00 - Number of parameters (encoder): 10563201
INFO - 04/25/22 13:12:40 - 0:00:00 - Number of parameters (decoder): 31278014
INFO - 04/25/22 13:33:07 - 0:00:00 - ============ Initialized logger ============
INFO - 04/25/22 13:33:07 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: False
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments\chart2text\run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: False
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 04/25/22 13:33:07 - 0:00:00 - The experiment will be stored in experiments\chart2text\run1
                                     
INFO - 04/25/22 13:33:07 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 04/25/22 13:33:07 - 0:00:00 - ============ Data summary ============
INFO - 04/25/22 13:33:07 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:33:07 - 0:00:00 - Removed 0 empty sentences.

INFO - 04/25/22 13:33:07 - 0:00:00 - Content-Selection Data -       5703
INFO - 04/25/22 13:33:07 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:33:07 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 04/25/22 13:33:07 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:33:07 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:33:07 - 0:00:00 - Para Data          -       5703
INFO - 04/25/22 13:33:07 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 04/25/22 13:33:07 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 04/25/22 13:33:07 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:33:07 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 04/25/22 13:33:07 - 0:00:00 - Encoder: TransformerEncoder(
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 04/25/22 13:33:07 - 0:00:00 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 04/25/22 13:33:07 - 0:00:00 - Number of parameters (encoder): 10563201
INFO - 04/25/22 13:33:07 - 0:00:00 - Number of parameters (decoder): 31278014
INFO - 04/25/22 13:33:58 - 0:00:00 - ============ Initialized logger ============
INFO - 04/25/22 13:33:58 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: False
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments\chart2text\run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: False
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 04/25/22 13:33:58 - 0:00:00 - The experiment will be stored in experiments\chart2text\run1
                                     
INFO - 04/25/22 13:33:58 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 04/25/22 13:33:58 - 0:00:00 - ============ Data summary ============
INFO - 04/25/22 13:33:58 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:33:58 - 0:00:00 - Removed 0 empty sentences.

INFO - 04/25/22 13:33:58 - 0:00:00 - Content-Selection Data -       5703
INFO - 04/25/22 13:33:58 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:33:58 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 04/25/22 13:33:58 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:33:58 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:33:58 - 0:00:00 - Para Data          -       5703
INFO - 04/25/22 13:33:58 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 04/25/22 13:33:58 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 04/25/22 13:33:58 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:33:58 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 04/25/22 13:33:58 - 0:00:00 - Encoder: TransformerEncoder(
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 04/25/22 13:33:58 - 0:00:00 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 04/25/22 13:33:58 - 0:00:00 - Number of parameters (encoder): 10563201
INFO - 04/25/22 13:33:58 - 0:00:00 - Number of parameters (decoder): 31278014
INFO - 04/25/22 13:34:35 - 0:00:00 - ============ Initialized logger ============
INFO - 04/25/22 13:34:35 - 0:00:00 - asm: False
                                     attention_dropout: 0.1
                                     batch_size: 6
                                     beam_size: 4
                                     clip_grad_norm: 5
                                     command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu --exp_id "run1"
                                     cs_step: True
                                     cuda: False
                                     dec_n_layers: 6
                                     dropout: 0.1
                                     early_stopping: True
                                     emb_dim: 512
                                     enc_n_layers: 1
                                     encoder_only: False
                                     encoder_positional_emb: False
                                     epoch_size: 1000
                                     eval_bleu: True
                                     eval_cs: False
                                     eval_only: False
                                     exp_id: run1
                                     exp_name: chart2text
                                     gelu_activation: True
                                     group_by_size: True
                                     label_smoothing: 0.05
                                     lambda_cs: 1
                                     lambda_lm: 0
                                     lambda_sm: 1
                                     length_penalty: 1.0
                                     lm_step: False
                                     max_batch_size: 0
                                     max_epoch: 81
                                     max_len: 100
                                     model_path: experiments\chart2text\run1
                                     n_heads: 8
                                     optimizer: adam,lr=0.0001
                                     reload_checkpoint: 
                                     reload_emb: 
                                     reload_model: 
                                     reload_model_strict: False
                                     save_periodic: 40
                                     share_inout_emb: True
                                     share_srctgt_emb: False
                                     sinusoidal_embeddings: False
                                     sm_step: True
                                     sm_step_with_cc_loss: False
                                     sm_step_with_cs_proba: False
                                     split_data: False
                                     stopping_criterion: 
                                     tokens_per_batch: -1
                                     train_cs_table_path: data/train/trainData.txt.pth
                                     train_sm_summary_path: data/train/trainSummary.txt.pth
                                     train_sm_table_path: data/train/trainData.txt.pth
                                     valid_summary_path: data/valid/validSummary.txt.pth
                                     valid_table_path: data/valid/validData.txt.pth
                                     validation_metrics: valid_mt_bleu
INFO - 04/25/22 13:34:35 - 0:00:00 - The experiment will be stored in experiments\chart2text\run1
                                     
INFO - 04/25/22 13:34:35 - 0:00:00 - Running command: python model/train.py --model_path experiments --exp_name chart2text --exp_id run1 --train_cs_table_path 'data/train/trainData.txt.pth' --train_sm_table_path 'data/train/trainData.txt.pth' --train_sm_summary_path 'data/train/trainSummary.txt.pth' --valid_table_path 'data/valid/validData.txt.pth' --valid_summary_path 'data/valid/validSummary.txt.pth' --cs_step True --lambda_cs 1 --sm_step True --lambda_sm 1 --label_smoothing '0.05' --sm_step_with_cc_loss False --sm_step_with_cs_proba False --share_inout_emb True --share_srctgt_emb False --emb_dim 512 --enc_n_layers 1 --dec_n_layers 6 --dropout '0.1' --save_periodic 40 --batch_size 6 --beam_size 4 --epoch_size 1000 --max_epoch 81 --eval_bleu True --sinusoidal_embeddings False --encoder_positional_emb False --gelu_activation True --validation_metrics valid_mt_bleu

INFO - 04/25/22 13:34:35 - 0:00:00 - ============ Data summary ============
INFO - 04/25/22 13:34:35 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:34:35 - 0:00:00 - Removed 0 empty sentences.

INFO - 04/25/22 13:34:35 - 0:00:00 - Content-Selection Data -       5703
INFO - 04/25/22 13:34:35 - 0:00:00 - Loading dataOld from data/train/trainData.txt.pth ...
INFO - 04/25/22 13:34:35 - 0:00:00 - Loading dataOld from data/train/trainSummary.txt.pth ...
INFO - 04/25/22 13:34:35 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:34:35 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:34:35 - 0:00:00 - Para Data          -       5703
INFO - 04/25/22 13:34:35 - 0:00:00 - Loading dataOld from data/valid/validData.txt.pth ...
INFO - 04/25/22 13:34:35 - 0:00:00 - Loading dataOld from data/valid/validSummary.txt.pth ...
INFO - 04/25/22 13:34:35 - 0:00:00 - Removed 0 empty sentences.
INFO - 04/25/22 13:34:35 - 0:00:00 - Removed 0 empty sentences.

DEBUG - 04/25/22 13:34:35 - 0:00:00 - Encoder: TransformerEncoder(
                                        (embeddings): Embedding(57885, 128, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (pred_layer): BinaryOutputLayer(
                                          (proj): Linear(in_features=512, out_features=1, bias=True)
                                          (proj_act): Sigmoid()
                                          (criterion): BCELoss()
                                        )
                                      )
DEBUG - 04/25/22 13:34:35 - 0:00:00 - Decoder: TransformerDecoder(
                                        (position_embeddings): Embedding(602, 512)
                                        (embeddings): Embedding(11198, 512, padding_idx=2)
                                        (layer_norm_emb): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        (attentions): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm1): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (ffns): ModuleList(
                                          (0): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (1): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (2): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (3): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (4): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                          (5): TransformerFFN(
                                            (lin1): Linear(in_features=512, out_features=2048, bias=True)
                                            (lin2): Linear(in_features=2048, out_features=512, bias=True)
                                          )
                                        )
                                        (layer_norm2): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (layer_norm15): ModuleList(
                                          (0): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (4): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                          (5): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
                                        )
                                        (encoder_attn): ModuleList(
                                          (0): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (1): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (2): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (3): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (4): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                          (5): MultiHeadAttention(
                                            (q_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (k_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (v_lin): Linear(in_features=512, out_features=512, bias=True)
                                            (out_lin): Linear(in_features=512, out_features=512, bias=True)
                                          )
                                        )
                                        (pred_layer): PredLayer(
                                          (proj): Linear(in_features=512, out_features=11198, bias=True)
                                        )
                                      )
INFO - 04/25/22 13:34:35 - 0:00:00 - Number of parameters (encoder): 10563201
INFO - 04/25/22 13:34:35 - 0:00:00 - Number of parameters (decoder): 31278014
